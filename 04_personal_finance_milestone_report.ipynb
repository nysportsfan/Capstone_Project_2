{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joshua Kim**\n",
    " \n",
    "**SpringBoard - Data Science Career Track**\n",
    "\n",
    "# Capstone Project 2: Topic-Modeling with /r/PersonalFinance\n",
    "\n",
    "***\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "0. [Introduction](#0.-Introduction)\n",
    "\n",
    "\n",
    "1. [Data Acquisition](#1.-Data-Acquisition)\n",
    "\n",
    "\n",
    "2. [Data Wrangling](#2.-Data-Wrangling)\n",
    "\n",
    "\n",
    "3. [Exploratory Data Analysis](#3.-Exploratory-Data-Analysis)\n",
    "\n",
    "\n",
    "4. [Next Steps](#Next-Steps)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    ">### The Problem\n",
    "\n",
    "One of the common problems that the everyday American faces is their financial situation. There are many different reasons for this, such as debt, student loans, the passing of a parent. According to a study of 1000 Americans over the age of 30 (conducted by [GuideVine](https://www.guidevine.com/newsroom/half-americans-age-30-cant-explain-401k/), less than half of Americans understood common financial concepts such as interest, bankruptcy, and inflation. More importantly, more than half felt \"lost\" when it came to having a secure long-term plan for their financial future. This highlights a re-occuring theme in our society where many people have trouble getting by with their finances and naturally some people seek help online. One of these resources is a subreddit called 'Personal Finance', which is hosted on the popular discussion-board website **[Reddit](https://www.reddit.com/)**.\n",
    "\n",
    "![s](https://github.com/nysportsfan/Personal_Finance_Subreddit/blob/master/Images/PersonalFinance%20Subreddit%20Screenshot.png?raw=true)\n",
    "\n",
    "**[/r/personalfinance](https://www.reddit.com/r/personalfinance/)** has become a place where many people are able to ask for help regarding their financial situations. A few common topics are savings, budgeting, taxes and debt. There are hundreds of different posts everyday from people of different ages, genders, locations and occupations. Frequent users of the subreddit often provide advice through comments and are able to upvote posts to help it gain more visibility on the front page of /r/personalfinance. However, the main feature we will be focusing on in this project is the \"flair\" feature which enables users to tag their submissions with a topic. For example, if I wrote a post asking advice on paying off my student loans, I would use the 'Debt' flair. This helps organize the hundreds of daily posts into different categories so that people will be able to search them more easily.\n",
    "\n",
    "There are 3 main problems with this:\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "1) <b>Not all users choose a flair for their post.</b> \n",
    "</div>\n",
    "\n",
    "In this scenario, the system will automatically tag the post with 'Other', which is used when the user either doesn't know what topic the post should fall under OR if the user does not assign a topic. This becomes a problem since this can result in many posts not having their proper flair and ruins the organized structure of searching by topic. The 'Other' flair was originally designed to contain posts that did not fall under the other topics but due to the popularity of /r/PersonalFinance (over 13 million subscribers), there are often many new users who do not how to either assign a topic (issue with the interface) or choose a topic (unfamiliar with financial categories). Because of this, the 'Other' flair is usually the most popular and common topic on the subreddit despite the fact that most of the posts actually do fall under another topic. \n",
    "\n",
    "It would be like someone throwing their plastic trash into a regular garbage bin instead of the recycle bin because they were unaware of what the recycle bin is for, they did not see it, or they did not care to throw it into the proper bin. If this was simply a rarity, it could be overlooked but if it becomes a common occurence, it would create a larger problem that would need to be remedied quickly. The same logic applies for properly flairing a post rather than leaving it to 'Other'. \n",
    "\n",
    "![s](http://ahs.mcnairycountyschools.com/uploads/5/2/7/9/52798193/9759366_orig.gif)\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "2) <b>There are posts which fit under multiple topics.</b>\n",
    "</div>\n",
    "\n",
    "Each post can only have a single flair thus forcing users to decide which topic best represents their question or idea. However this can have several repercussions. First, the way a user decides on the topic is very subjective and may not be the best fit. Second, if they don't know which topic to pick for their post, they may decide to simply classify it under 'Other', which goes back to the first problem. \n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "3) <b>The current set of available topics may not contain the best variety or be most efficient.</b> \n",
    "</div>\n",
    "\n",
    "This is a more interesting problem and can be an analytical problem for another project because it is difficult to ascertain how efficient it is to use the current batch of flairs. One question is whether we should add more topics in order to reduce the number of 'Other' posts and to also make it easier for users to identify the proper flair for their submissions. For example, should we include a new 'Student' topic for questions pertaining to student loans, textbook savings, getting a campus job, etc. Another question is whether or not we can *remove* any of the current topics and replace it with something else. This is not to say that the current topics are bad; in fact, using an eye-test would be enough for most people to be satisfied with them but how do we know whether or not they are the most efficient set of topics? \n",
    "\n",
    "To sum up these 3 main problems into a single question: \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>How can we create an efficient set of topics and automatically assign a single one for each post?</b>\n",
    "</div>\n",
    "\n",
    "---------------------------------\n",
    "\n",
    ">### The Project\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>The main goal of this project is to create a topic-modeling algorithm that will generate a new set of topics for /r/personalfinance posts.</b>\n",
    "</div>\n",
    "\n",
    "This will consist of 4 stages to accomplish this task.\n",
    "\n",
    "**1) [Data Acquisition](#1.-Data-Acquisition)**: This is often the most important step in any data science project (along with cleaning the data). We will first need to acquire the relevant data from /r/personalfinance using pushshift.io's API as well as Reddit's official API. \n",
    "\n",
    "**2) [Data Wrangling](#2.-Data-Wrangling)**: After we extract the data, we will need to clean it and prepare it for analysis. Natural Language Processing projects require pre-processing the text so that the machine readily able to interpret it after vectorization. Any incoherent symbols or punctuation will make the model performance worse so this is crucial before creating the model.\n",
    "\n",
    "**3) [Exploratory Data Analysis](#3.-Exploratory-Data-Analysis)**: We will begin to explore the data and observe any interesting trends that can provide a direction or foundation for how we proceed with the project. Natural Language Processing projects are typically limited in how much EDA can be performed because of the lack of features. After all, we are mainly dealing with text!\n",
    "\n",
    "**4) [Machine Learning](#4.-Machine-Learning)**: Let us split the problem statement into 2 parts: \n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>First, how can we create topics based on the posts' text and assign one to each post?</b>\n",
    "</div>\n",
    "\n",
    "- The main type of algorithm we seek to use is a topic model, which discovers topics based on hidden semantic structures in the various documents of a corpus (which is a large unstructured set of text). More specifically, this will require the implementation of Latent Dirichlet Allocation, a generative statistical model that works well in topic modeling by mapping each document to a number of different topics and each topic to a number of different words. To evaluate how well a generated set of topics does in clustering the documents, we will be using 2 metrics: Perplexity and Coherence (this will be discussed in greater detail in the machine learning section).\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Second, how do we evaluate whether or not these topics are efficient?</b>\n",
    "</div>\n",
    "\n",
    "- To answer this question, we must think about how we can define an \"efficient\" topic. Remember that we want this model to be able to not only do well in creating topics for the fitted data, but also do a good job in assigning topics for *new* posts. The problem we face is that there are many incidents where a user does not select a topic for their own posts. Therefore this model should help resolve this issue by automatically creating a topic for these new posts. \n",
    "\n",
    "\n",
    "- The best way to evaluate the efficiency of the new topics that we generate from our topic model is by measuring the accuracy of predicting the labels. In other words, we will be creating a new classification problem where the feature variables are the text for each post and the response variable is the generated topic for each post. We then use different machine learning algorithms (such as Naive Bayes and Stochastic Gradient Descent) to create a good model that can predict the topics for a given row. Finally, we use the Accuracy, Precision and Recall metrics to evaluate how well the model performed.\n",
    "\n",
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Note: We did not use inferential statistics to do hypothesis testing or to observe relationships between the feature and response variables. This is because our feature variables are the vectorized form of words and there are thousands of different words in our usable dictionary, which would make any statistical insight practically meaningless.\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    ">### The Clients\n",
    "\n",
    "1) **/r/PersonalFinance Users** will be interested in having a new system that could automatically determine a proper topic for their submission so that they don't have to choose one themselves. Whether they are too lazy, don't know which topic to select This will also help other people who search posts by topic. For example, if someone was interested in reading advice for people struggling with low income, they could simply navigate through the 'Planning' topic. \n",
    "\n",
    "![s](https://watermarked.cutcaster.com/cutcaster-photo-100747197-Cartoon-boy-working-with-computer.jpg)\n",
    " \n",
    " \n",
    "2) **/r/PersonalFinance Moderators** will benefit from having a clearer structure for all submissions. Many of the popular subreddits insist on their members tagging their posts with flairs because of the convenience and organization it provides to the subreddit as a whole (sometimes this is even a requirement). One of the problems that moderators face with their subreddits is the influx of re-occuring questions. Many subreddits will ask users to first search for similar posts before submitting their own because it helps to reduce the number of the same posed questions. By having a better topic system, this will help the search feature because there will be fewer 'Other' posts and more of the specific topics. \n",
    " \n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition\n",
    "\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    ">**Obtaining the data**\n",
    "\n",
    "For this report, I created my own dataset through 2 steps:\n",
    "\n",
    "**First**, we extract the submission information using the __[pushshift.io Reddit API](https://github.com/pushshift/api)__. It gives us the ability to search through Reddit data and create data aggregations. Using sqlite was necessary as there are many different rows of data to collect.\n",
    "\n",
    "We want to first specify the date ranges from which we can extract the data. This is important so that we can attach the date ranges to the pushshift url. For example, if we wanted to extract the newest post submissions as of August 25th 2018 for /r/nba (a subreddit for fans of the National Basketball Association), we would first convert August 25th 2018 11:59:59 PM into datetime format and then into a timestamp using the .timetuple() method. We then add the subreddit and timestamp as strings in the url. \n",
    "\n",
    "This would be the resulting url: http://api.pushshift.io/reddit/search/submission/?subreddit=personalfinance&size=500&before=1535255999\n",
    "\n",
    "Take a quick look!\n",
    "\n",
    "![s](https://github.com/nysportsfan/Personal_Finance_Subreddit/blob/master/Images/Pushshift%20IO%20Example.png?raw=true)\n",
    "\n",
    "Notice that this will provide the 500 most recent posts as of August 25th 11:59PM because we specified size = 500. We then use the requests module to use its builtin JSON decoder and extract the relevant data from each pull. To get the next 500 most recent posts, we simply do another pull by replacing the timestamp such that it comes right after the date and time of the 500th post. \n",
    "\n",
    "**Second**, I used the __[official Reddit API](https://praw.readthedocs.io/en/latest/)__ to extract information that is not provided through the pushshift.io Reddit API. In this case, we extracted the flair (which will act as the \"topics\" of each submission) and self-text information (which are the body of text for each submission). The reason we cannot simply use the official Reddit API to extract all the submission data is because they limit the number of submissions (1000) you are allowed to extract as well as the date range. For example, if we extract the 1000 most recent posts on a subreddit, we cannot extract the next 1000 most recent posts since we cannot specify the date range. We will have a dataset that contains approximately 100,000 rows of data. Each row represents a submission in the Personal Finance subreddit. The features will include:\n",
    "\n",
    "- title\n",
    "\n",
    "- date (the date at which the post was submitted)\n",
    "\n",
    "- time (the time at which the post was submitted)\n",
    "\n",
    "- upvotes (number of upvotes for each submission)\n",
    "\n",
    "- id (submission ID)\n",
    "\n",
    "- topic (flair)\n",
    "\n",
    "- self-text (the text information within each post)\n",
    "\n",
    "To see the code and more annotation, check out the [Data Acquisition](https://github.com/nysportsfan/Personal_Finance_Subreddit/blob/master/01_pf_data_wrangling.ipynb) notebook.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Note: This process of scraping data is very computationally expensive. Consider using PaperSpace or Google Cloud to speed up the process.\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Wrangling\n",
    "\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    ">**Inspecting the Dataset**\n",
    "\n",
    "Let's begin by observing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23182 entries, 0 to 23181\n",
      "Data columns (total 7 columns):\n",
      "title        23182 non-null object\n",
      "date         23182 non-null object\n",
      "time         23182 non-null object\n",
      "upvotes      23182 non-null int64\n",
      "id           23182 non-null object\n",
      "topic        23182 non-null object\n",
      "self_text    23182 non-null object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pf_files import *\n",
    "\n",
    "df = pd.read_pickle(r'C:\\Users\\Joshua\\Pickle_files\\df')\n",
    "first_df = df[['title','date','time','upvotes','id','topic','self_text']]\n",
    "\n",
    "first_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>self_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ways to make extra side money?</td>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>12:57 PM</td>\n",
       "      <td>1</td>\n",
       "      <td>9h6whn</td>\n",
       "      <td>unknown</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Year UPDATE) Legally blind, going homeless, h...</td>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>12:56 AM</td>\n",
       "      <td>16</td>\n",
       "      <td>9h29g7</td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19, being kicked out</td>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>12:55 PM</td>\n",
       "      <td>2</td>\n",
       "      <td>9h6vyv</td>\n",
       "      <td>Other</td>\n",
       "      <td>So i just found out last night the home ive be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online Savings Account?</td>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>12:54 PM</td>\n",
       "      <td>1</td>\n",
       "      <td>9h6vs4</td>\n",
       "      <td>Saving</td>\n",
       "      <td>Hello! Looking for recommendations for an onli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tools for Managing Incomes and Expenses</td>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>12:52 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>9h6v48</td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        date      time  \\\n",
       "0                     Ways to make extra side money?  2018-09-19  12:57 PM   \n",
       "1  (Year UPDATE) Legally blind, going homeless, h...  2018-09-19  12:56 AM   \n",
       "2                               19, being kicked out  2018-09-19  12:55 PM   \n",
       "3                            Online Savings Account?  2018-09-19  12:54 PM   \n",
       "4            Tools for Managing Incomes and Expenses  2018-09-19  12:52 PM   \n",
       "\n",
       "   upvotes      id    topic                                          self_text  \n",
       "0        1  9h6whn  unknown                                                     \n",
       "1       16  9h29g7    Other                                                     \n",
       "2        2  9h6vyv    Other  So i just found out last night the home ive be...  \n",
       "3        1  9h6vs4   Saving  Hello! Looking for recommendations for an onli...  \n",
       "4        0  9h6v48    Other                                                     "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Creating the Text column**\n",
    "\n",
    "Rather than having the title and the self-text in separate columns, it would be more convenient to have both in a single column. We do so by concatenating the 2 columns into a new column called 'text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ways to make extra side money?</td>\n",
       "      <td></td>\n",
       "      <td>Ways to make extra side money?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Year UPDATE) Legally blind, going homeless, h...</td>\n",
       "      <td></td>\n",
       "      <td>(Year UPDATE) Legally blind, going homeless, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19, being kicked out</td>\n",
       "      <td>So i just found out last night the home ive be...</td>\n",
       "      <td>19, being kicked out So i just found out last ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online Savings Account?</td>\n",
       "      <td>Hello! Looking for recommendations for an onli...</td>\n",
       "      <td>Online Savings Account? Hello! Looking for rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tools for Managing Incomes and Expenses</td>\n",
       "      <td></td>\n",
       "      <td>Tools for Managing Incomes and Expenses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                     Ways to make extra side money?   \n",
       "1  (Year UPDATE) Legally blind, going homeless, h...   \n",
       "2                               19, being kicked out   \n",
       "3                            Online Savings Account?   \n",
       "4            Tools for Managing Incomes and Expenses   \n",
       "\n",
       "                                           self_text  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  So i just found out last night the home ive be...   \n",
       "3  Hello! Looking for recommendations for an onli...   \n",
       "4                                                      \n",
       "\n",
       "                                                text  \n",
       "0                    Ways to make extra side money?   \n",
       "1  (Year UPDATE) Legally blind, going homeless, h...  \n",
       "2  19, being kicked out So i just found out last ...  \n",
       "3  Online Savings Account? Hello! Looking for rec...  \n",
       "4           Tools for Managing Incomes and Expenses   "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['title','self_text','text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Convert Time and Dates into datetime format**\n",
    "\n",
    "Since the data is given as a string, we will need to convert the time and date into datatime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>12:57 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>12:56 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>12:55 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>12:54 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>12:52 PM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      time\n",
       "0  2018-09-19  12:57 PM\n",
       "1  2018-09-19  12:56 AM\n",
       "2  2018-09-19  12:55 PM\n",
       "3  2018-09-19  12:54 PM\n",
       "4  2018-09-19  12:52 PM"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['date','time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Replace missing topics**\n",
    "\n",
    "Some posts have missing topics and self-text. This is because the post was either deleted by the user or removed by a moderator. Since we plan on creating our own topics, it is not a big deal if we are missing the topic. Let's fill missing topics by categorizing them under 'unknown'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Debt                 1256\n",
       "Other                1205\n",
       "Credit               1048\n",
       "Investing             859\n",
       "Retirement            824\n",
       "Employment            724\n",
       "Housing               709\n",
       "unknown               651\n",
       "Auto                  560\n",
       "Planning              526\n",
       "Saving                524\n",
       "Taxes                 509\n",
       "Budgeting             405\n",
       "Insurance             380\n",
       "Meta                    2\n",
       "THIS IS A SPAMMER       1\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(r'C:\\Users\\Joshua\\Downloads\\Data\\reddit\\reddit_pf.csv', engine = 'python', index_col=0)\n",
    "\n",
    "df1.fillna('unknown').topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Replace outlier topics**\n",
    "\n",
    "As we can see from the previous point, there are two outlier topics: 2 instances of 'Meta' and 1 instance of 'THIS IS A SPAMMER'. Since these are clearly not intended to be part of the set of topics, we should replace these with 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Debt          2819\n",
       "Other         2749\n",
       "Credit        2447\n",
       "Investing     1910\n",
       "Retirement    1814\n",
       "Employment    1688\n",
       "Housing       1574\n",
       "unknown       1396\n",
       "Auto          1303\n",
       "Planning      1229\n",
       "Saving        1200\n",
       "Taxes         1164\n",
       "Budgeting     1012\n",
       "Insurance      877\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Text pre-processing**\n",
    "\n",
    "As the saying goes: __[\"Garbage In, Garbage Out\"](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out)__. Before tokenizing the text and feeding it into the machine, we will need to clean up the data. To do so, we will create a function that will perform the following steps:\n",
    "\n",
    "- **Lowercase** the words so that the model will not differentiate capitalized words from other words.\n",
    "\n",
    "- **Remove numbers/digits** since the model is interpreting *text* not numbers.\n",
    "\n",
    "- **Remove punctuation** since it is not important for the context.\n",
    "\n",
    "- **Strip white space** since empty strings could be interpreted as text and we want to avoid that.\n",
    "\n",
    "- **Remove stopwords**, which are general words that are very frequent in the English dictionary (ex. because, such, so). Here is a __[list of some common stopwords](https://www.ranks.nl/stopwords)__.\n",
    "\n",
    "- **Remove noise** that is not picked up through the other cleaning methods. This step can come either before or after tokenization and normalization, or both (ex. dropping words that are less than 2 characters long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ways to make extra side money?</td>\n",
       "      <td>ways make extra side money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Year UPDATE) Legally blind, going homeless, h...</td>\n",
       "      <td>year update legally blind going homeless one j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19, being kicked out So i just found out last ...</td>\n",
       "      <td>kicked found last night home ive staying going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online Savings Account? Hello! Looking for rec...</td>\n",
       "      <td>online savings account hello looking recommend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tools for Managing Incomes and Expenses</td>\n",
       "      <td>tools managing incomes expenses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                    Ways to make extra side money?    \n",
       "1  (Year UPDATE) Legally blind, going homeless, h...   \n",
       "2  19, being kicked out So i just found out last ...   \n",
       "3  Online Savings Account? Hello! Looking for rec...   \n",
       "4           Tools for Managing Incomes and Expenses    \n",
       "\n",
       "                                          clean_text  \n",
       "0                         ways make extra side money  \n",
       "1  year update legally blind going homeless one j...  \n",
       "2  kicked found last night home ive staying going...  \n",
       "3  online savings account hello looking recommend...  \n",
       "4                    tools managing incomes expenses  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text','clean_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Check for missing text**\n",
    "\n",
    "After pre-processing the text, there are some cases where it completely removes all text. We will remove these rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>1099 or W2?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3646</th>\n",
       "      <td>401Ks?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>401k</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4705</th>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text clean_text\n",
       "2845  1099 or W2?            \n",
       "3646       401Ks?            \n",
       "4592         401k            \n",
       "4705            T            "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[[2845, 3646, 4592, 4705], ['text', 'clean_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Setting a word limit**\n",
    "\n",
    "We also limited the number of words in the text since the longer the text, the longer the computation time. We did consider using the number of characters but the number of words seemed to do a better job. 150 was used as the word limit for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Word Length: 66.79087222845311\n",
      "\n",
      "Median Word length: 49.0\n",
      "\n",
      "Percentage of texts with word count less than 150: 89.47%\n"
     ]
    }
   ],
   "source": [
    "txt_wrd_mean = df['clean_text'].apply(lambda x: x.split()).map(len).mean()\n",
    "txt_wrd_median = df['clean_text'].apply(lambda x: x.split()).map(len).median()\n",
    "\n",
    "print('Mean Word Length: {}\\n\\nMedian Word length: {}\\n'.format(txt_wrd_mean, txt_wrd_median))\n",
    "\n",
    "word_count_ = len(df[df['clean_text'].apply(lambda x: x.split()).map(len)<150])\n",
    "total_len = len(df)\n",
    "\n",
    "print('Percentage of texts with word count less than 150: {0:.2f}%'.format((word_count_/total_len)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Tokenization**\n",
    "\n",
    "In order to better analyze individual words, we will need to *tokenize* the documents (or in this case, the submission titles) into pieces of words. By doing so, we will be able to use the various NLP libraries to further dissect the tokens (ex. spaCy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Lemmatization**\n",
    "\n",
    "After tokenizing the data, we will need to normalize the text through lemmatization or stemming. Lemmatization is typically a better method since it returns the canonical forms based on a word's lemma. However, this process takes much more time compared to stemming the words, which simply removes the affixes of a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ways to make extra side money?</td>\n",
       "      <td>ways make extra side money</td>\n",
       "      <td>[way, make, extra, side, money]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Year UPDATE) Legally blind, going homeless, h...</td>\n",
       "      <td>year update legally blind going homeless one j...</td>\n",
       "      <td>[year, update, legally, blind, go, homeless, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19, being kicked out So i just found out last ...</td>\n",
       "      <td>kicked found last night home ive staying going...</td>\n",
       "      <td>[kick, find, last, night, home, have, stay, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online Savings Account? Hello! Looking for rec...</td>\n",
       "      <td>online savings account hello looking recommend...</td>\n",
       "      <td>[online, saving, account, hello, look, recomme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tools for Managing Incomes and Expenses</td>\n",
       "      <td>tools managing incomes expenses</td>\n",
       "      <td>[tool, manage, income, expense]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                    Ways to make extra side money?    \n",
       "1  (Year UPDATE) Legally blind, going homeless, h...   \n",
       "2  19, being kicked out So i just found out last ...   \n",
       "3  Online Savings Account? Hello! Looking for rec...   \n",
       "4           Tools for Managing Incomes and Expenses    \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                         ways make extra side money   \n",
       "1  year update legally blind going homeless one j...   \n",
       "2  kicked found last night home ive staying going...   \n",
       "3  online savings account hello looking recommend...   \n",
       "4                    tools managing incomes expenses   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0                    [way, make, extra, side, money]  \n",
       "1  [year, update, legally, blind, go, homeless, o...  \n",
       "2  [kick, find, last, night, home, have, stay, go...  \n",
       "3  [online, saving, account, hello, look, recomme...  \n",
       "4                    [tool, manage, income, expense]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text','clean_text', 'lemmatized_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Named Entity Recognition**\n",
    "\n",
    "Named entities are real-world objects that have a name, such as a person, country, or company. spaCy is able to recognize different types of named entities in a document and can return features such as the label (ex. ORG - organization, GPE - geopolitical entity). Not all documents will contain named entities since they may not reference any. While this is not a feature we plan to incorporate in the topic model, it will be interesting for observation in the exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>named_entities</th>\n",
       "      <th>entity_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[one]</td>\n",
       "      <td>[CARDINAL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[last night, returning make week]</td>\n",
       "      <td>[TIME, DATE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[first]</td>\n",
       "      <td>[ORDINAL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      named_entities entity_labels\n",
       "0                                 []            []\n",
       "1                              [one]    [CARDINAL]\n",
       "2  [last night, returning make week]  [TIME, DATE]\n",
       "3                            [first]     [ORDINAL]\n",
       "4                                 []            []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['named_entities', 'entity_labels']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The goal of this notebook is to explore the data extracted from Reddit's Personal Finance subreddit, including information such as:\n",
    "\n",
    "- Text of submission\n",
    "\n",
    "- Date of submission\n",
    "\n",
    "- Topic\n",
    "\n",
    "- Number of Upvotes\n",
    "\n",
    "- Length of text\n",
    "\n",
    "Using Natural Language Processing tools and data visualization, we aim to learn any obvious and underlying trends from the submission information. However, it should be worth mentioning that there won't that many details to uncover compared to other types of datasets due to the fact that the model's only input will be text. Therefore many of the following findings may not be relevant to creating the topics but nonetheless, they are still interesting to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Which are the most common topics?**\n",
    "\n",
    "Debt is the most common topic in the personal finance subreddit, followed by Other and Credit. This indicates that debt is a major concern for many of the redditors (users) and they make submissions in order to seek advice.\n",
    "\n",
    "As discussed before, Other would include posts that either haven't been given a topic or didn't match the main topics. With more data, it could be possible that Other is the most frequent topic instead of Debt.\n",
    "\n",
    "Investment and Retirement are also similar in their counts while also being very similar in their functions.\n",
    "\n",
    "![s](https://github.com/nysportsfan/Personal_Finance_Subreddit/blob/master/Images/PersonalFinance%20Topic%20Count.png?raw=true)\n",
    "\n",
    "> **Which are the top 10 most popular submissions?**\n",
    "\n",
    "By popular, we mean which submissions had the most user upvotes. \n",
    "\n",
    "There are a variety of different subjects in the top 10 most popular posts, ranging from scams to dealing with the loss of a family member. However, 3 of the top 10 include the word \"scam\", showing that this may be a popular subject in /r/personalfinance and could happen often. This could be a common issue among many people living in the US based on the number of upvotes.\n",
    "\n",
    "While some posts are seeking advice, others provide advice such as '\"Hidden\" costs of buying a home and how to prepare for them.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"IRS will allow employers to match their employees' student loan repayments\",\n",
       " \"If you can't get your emergency fund to grow because of emergencies that keep coming up, you're still doing a good job.\",\n",
       " 'Your amazon store card is probably scamming you',\n",
       " 'A story of how I just got out of paying a $1500 bill, why you should NEVER blindly trust a Debt collection agency, and ALWAYS request proof of a Debt owed. ðŸ˜‚ðŸ˜‚ðŸ˜‚',\n",
       " 'Girlfriend had some standard medical tests done. The clinic apparently waited too long to bill her insurance company and insurance is declining to pay them. Now, the clinic is trying to bill girlfriend for full amount and threatening to go to collections. What are her options?',\n",
       " \"If the only reason you pay for Amazon Prime is because of 2-day shipping, there is a good chance it's a smarter financial move for you to cancel it than continue.\",\n",
       " 'So I fell for a scam yesterday and it still angers me.',\n",
       " 'Credit freezes are now free. Starting today.',\n",
       " \"I'm 18 and I will be homeless soon. Please help.\",\n",
       " 'Wife passed away. I would truly appreciate some help figuring out where I stand.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 most popular headlines\n",
    "[title for title in df.sort_values(by='upvotes', ascending=False)['title'].values[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Which are the most common tokenized words?**\n",
    "\n",
    "In the plot below, we can see the frequency of the top 100 most common tokens with ticks on intervals of 10.\n",
    "\n",
    "The word 'pay' has been used over 20,000 times in the corpus of text. After the first 10 tokens, the frequency that a word is used decreases very quickly. The words 'account' and 'payment' are also fairly common and transferable among many different topics which is why it is logical for them to have high counts. Towards the end of the graph, we can start to see more topic-specific words such as 'rent' and 'invest'.\n",
    "\n",
    "![s](https://github.com/nysportsfan/Personal_Finance_Subreddit/blob/master/Images/PersonalFinance%20Token%20Count.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What does the distribution of the text length look like?**\n",
    "\n",
    "As we can see, the histogram of the distribution of the text length is abnormally right-skewed with a strong kurtosis. While the graph peaks around 75-100 characters, it is difficult to tell where the mean of the data is if we only look at the graph. In reality, the mean number of characters for the posts is 602, which is far from the peak of the data. There is a strong peak but after about 150 words is a very flat-lined distribution where it gradually increases up until 600 and then begins to decline. \n",
    "\n",
    "One interpretation of this graph can be that there are many new users who simply want to ask a quick question without going into much detail. For example, a person may want to ask about whether they should get a Visa or MasterCard credit card. These types of posts are very common where people do not have to adequately describe their financial situation to get advice.\n",
    "\n",
    "Another interpretation is that many of the posts get deleted, resulting in the self-text disappearing and significantly reducing the number of characters in the total text. This is certainly plausible because of how many subscribers /r/personalfinance has. The more popular a subreddit becomes, the more common it is for new users to disregard the subreddit's rules and consequently, a higher chance that a moderator removes their post. In fact, the mean number of characters for posts under the 'unknown' topic (which is for posts that have been deleted or removed) is 130, which is well below the average of 602.\n",
    "\n",
    "![s](https://github.com/nysportsfan/Personal_Finance_Subreddit/blob/master/Images/PersonalFinance%20Text%20Length%20Distribution.png?raw=true)\n",
    "\n",
    "While this plot describes how the distribution is like for all posts, let's also take a look at the distribution by topic.\n",
    "\n",
    "We can see depending on the topic, the kurtosis of the histogram can vary greatly. For example, the histograms for 'Other' and 'unknown' have much heavier weight on their tails compared to the other topics such as 'Budgeting', which is appears very flat. Besides their peaks, the histograms look very similar to one another which means that outside of 'unknown', the topics have very similar distributions. \n",
    "\n",
    "![s](https://github.com/nysportsfan/Personal_Finance_Subreddit/blob/master/Images/PersonalFinance%20Text%20Length%20Distribution1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Most popular tokens by POS (Part-of-Speech) Tagging**\n",
    "\n",
    "Part of Speech tagging is marking a word based on its __[part of speech](https://en.wikipedia.org/wiki/Part_of_speech)__. Examples can include noun, verb, and pronoun. It was initially done by hand (ex. [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus)) but there are now many different algorithms that can be used to decipher a POS tag for any given document. spaCy is a python library that has one such algorithm that allows us to obtain the POS for a given word. Let's take advantage of this by extracting singular and plural nouns from each document. \n",
    "\n",
    "![s](https://github.com/nysportsfan/Personal_Finance_Subreddit/blob/master/Images/PersonalFinance%20Noun%20Token%20Count.png?raw=true)\n",
    "\n",
    "We see that the most common token is 'credit', not 'pay' like we had seen before. This hints at the fact that 'pay' is mainly used as a verb rather than a noun. Another interesting finding is that there are many date-based words such as 'year' and 'month'. Most likely this is because many people are describing their financial circumstances in the context of a year or several months. Other common tokens strongly suggest which topics they're part of. For example: \n",
    "\n",
    "- $'car' -> Auto$\n",
    "\n",
    "- $'debt' -> Debt$\n",
    "\n",
    "- $'job' -> Employment$\n",
    "\n",
    "- $'loan' -> Debt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**What are the most common named entities?**\n",
    "\n",
    "By far the most common types of named entity are time and date entities. This indicates that most users place a great amount of detail on the context of time when they are describing their financial issues. \n",
    "\n",
    "![s](https://github.com/nysportsfan/Personal_Finance_Subreddit/blob/master/Images/PersonalFinance%20NER.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Sentiment Analysis**\n",
    "\n",
    "While observing the relative sentiments of tokens is not particularly useful in determining the topics of the posts, it is nonetheless interesting to observe due to some surprising (and humorous) results. We can apply it to our text through the SentimentIntensityAnalyzer function from nltk's sentiment module.\n",
    "\n",
    "'Credit' is easily the most common positive word from the dataset, but this does beg the queston of whether or not it's truly positive in the context of the text. For example, what if they are referring to credit cards; would the word 'credit' still be considered positive? Similarly, the word 'interest' is the second most common positive word but interest in the financial context is probably negative! \n",
    "\n",
    "![s](https://github.com/nysportsfan/Personal_Finance_Subreddit/blob/master/Images/PersonalFinance%20Positive.png?raw=true)\n",
    "\n",
    "On the flip side, we see that 'debt' is the most common negative word and this isn't surprising since it's also the most popular topic on /r/personalfinance. Whether or not the word 'pay' is necessarily negative can be debatable in this context since it could simply be neutral. For example, if someone has to \"pay\" for a cup of coffee, it's not necessarily negative since they are making a transaction to buy a drink for consumption. \n",
    "\n",
    "![s](https://github.com/nysportsfan/Personal_Finance_Subreddit/blob/master/Images/PersonalFinance%20Negative.png?raw=true)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we have completed the Data Acquisition, Data Wrangling and Exploratory Data Analysis sections, we will be focusing on building the topic model with Latent Dirichlet Allocation and then evaluating the topics through classification algorithms, such as Multinomial Naive Bayes, Stochastic Gradient Descent, and Multi-Class Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
