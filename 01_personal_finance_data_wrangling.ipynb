{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Finance Subreddit Capstone Project\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "The goal of this notebook is to extract data containing information about the personal finance subreddit from the pushshift.io Reddit API, select interesting features that can be relevant to the project and clean the dataset so that it is ready for data exploration.\n",
    "\n",
    "- **Import the necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import string\n",
    "import time\n",
    "import datetime\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition\n",
    "\n",
    "- **Extract the data from reddit using pushshift.io's API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "sub    = 'personalfinance'     # name of the subreddit you would like to scrape\n",
    "after  = '2018-08-10'    # earliest date that will be scraped\n",
    "before = '2018-08-25'    # latest date that will be scraped\n",
    "fast   = True           # True will be faster, won't pull upvote ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate sqlite\n",
    "sql = sqlite3.connect('personalfinance_.db')\n",
    "cur = sql.cursor()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS posts (name TEXT, title TEXT, readable_utc TEXT, permalink TEXT, domain TEXT, url TEXT, author TEXT, score TEXT, upvote_ratio TEXT, num_comments TEXT)')\n",
    "sql.commit()\n",
    "print('Loaded SQL Database and Tables')\n",
    "\n",
    "# Convert the specified dates to strptime\n",
    "after = time.mktime(datetime.datetime.strptime(after, '%Y-%m-%d').timetuple())\n",
    "after = int(after)\n",
    "readable_after = time.strftime('%d %b %Y %I:%M %p', time.localtime(after))\n",
    "before = time.mktime(datetime.datetime.strptime(before, '%Y-%m-%d').timetuple())\n",
    "before = int(before) + 86399\n",
    "readable_before = time.strftime('%d %b %Y %I:%M %p', time.localtime(before))\n",
    "print('Searching for posts between ' + readable_after + ' and ' + readable_before + '.')\n",
    "currentDate = before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pushshift will allow us to retrieve valuable information from reddit submissions including:\n",
    "- Submission ID\n",
    "- Submission title\n",
    "- Submission date\n",
    "- Submission permalink\n",
    "- Submission domain\n",
    "- Submission url\n",
    "- Submission author\n",
    "- Submission score (upvotes)\n",
    "- Submission upvote_ratio (ratio of upvotes to downvotes)\n",
    "- Submission number of comments\n",
    "\n",
    "However, it does not provide us with the flair information.\n",
    "\n",
    "NOTE: This process is extremely computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a new full pull from Pushshift\n",
    "def newpull(thisBefore):\n",
    "    global currentDate\n",
    "    readable_thisBefore = time.strftime('%d %b %Y %I:%M %p', time.localtime(thisBefore))\n",
    "    print('Searching posts before ' + str(readable_thisBefore))\n",
    "    url = 'http://api.pushshift.io/reddit/search/submission/?subreddit=' + sub + '&size=500&before=' + str(thisBefore)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print('    Discussion: HTML Error - ', response.status_code)\n",
    "        time.sleep(60)\n",
    "        return\n",
    "    curJSON = response.json()\n",
    "\n",
    "    # Update each Pushshift result with Reddit data\n",
    "    for child in curJSON['data']:\n",
    "\n",
    "        # Check to see if already added\n",
    "        name = str(child['id'])\n",
    "        cur.execute('SELECT * FROM posts WHERE name == ?', [name])\n",
    "        if cur.fetchone():\n",
    "            print(str(child['id']) + ' skipped (already in database)')\n",
    "            continue\n",
    "\n",
    "        # If not, get more data\n",
    "        if fast is True:\n",
    "            searchURL = 'http://reddit.com/by_id/t3_'\n",
    "        else:\n",
    "            searchURL = 'http://reddit.com/'\n",
    "        url = searchURL + str(name) + '.json'\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print('    Discussion: HTML Error - ', response.status_code)\n",
    "            time.sleep(60)\n",
    "            break\n",
    "        postJSON = response.json()\n",
    "        if fast is True:\n",
    "            jsonStart = postJSON\n",
    "        else:\n",
    "            jsonStart = postJSON[0]\n",
    "\n",
    "        # Check to see if Date has passed\n",
    "        global currentDate\n",
    "        created_utc = jsonStart['data']['children'][0]['data']['created_utc']\n",
    "        currentDate = int(created_utc)\n",
    "        if currentDate <= after:\n",
    "            break\n",
    "\n",
    "        # If not, process remaining data\n",
    "        try:\n",
    "            title = str(jsonStart['data']['children'][0]['data']['title'])  # Checks for emojis and other non-printable characters\n",
    "        except UnicodeEncodeError:\n",
    "            title = ''.join(c for c in str(jsonStart['data']['children'][0]['data']['title']) if c in string.printable)\n",
    "        readable_utc = time.strftime('%d %b %Y %I:%M %p', time.localtime(created_utc))\n",
    "        permalink    = (str(jsonStart['data']['children'][0]['data']['permalink']))\n",
    "        domain       = (str(jsonStart['data']['children'][0]['data']['domain']))\n",
    "        url          = (str(jsonStart['data']['children'][0]['data']['url']))\n",
    "        author       = (str(jsonStart['data']['children'][0]['data']['author']))\n",
    "        score        = (str(jsonStart['data']['children'][0]['data']['score']))\n",
    "        num_comments = (str(jsonStart['data']['children'][0]['data']['num_comments']))\n",
    "        if fast is True:\n",
    "            upvote_ratio = 0\n",
    "        else:\n",
    "            upvote_ratio = (str(jsonStart['data']['children'][0]['data']['upvote_ratio']))\n",
    "\n",
    "        # Write it to SQL Database\n",
    "        cur.execute('INSERT INTO posts VALUES(?,?,?,?,?,?,?,?,?,?)', [name, title, readable_utc, permalink, domain, url, author, score, upvote_ratio, num_comments])\n",
    "        sql.commit()\n",
    "\n",
    "# Run the newpull\n",
    "while currentDate >= after:\n",
    "    newpull(currentDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have pulled the data from pushshift, we will need to create a dataframe which will store the relevant information (title, date, time, upvotes, id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconnect to sqlite\n",
    "connection = sqlite3.connect(\"personalfinance_.db\") \n",
    "  \n",
    "# Cursor object \n",
    "crsr = connection.cursor() \n",
    "  \n",
    "# Execute the command to fetch all the data from the table posts \n",
    "crsr.execute(\"SELECT * FROM posts\")  \n",
    "  \n",
    "# Store all the fetched data in the ans variable \n",
    "ans= crsr.fetchall()  \n",
    "\n",
    "# Create empty dataframe\n",
    "columns = ['title']\n",
    "index = range(0,2)\n",
    "df = pd.DataFrame(index = index, columns = columns)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Create new columns and extract the relevant data \n",
    "for n, i in enumerate(ans):\n",
    "    # Create title column\n",
    "    df.loc[n , 'title'] = i[1]\n",
    "    # Create date column\n",
    "    df.loc[n , 'date'] = i[2][:-8]\n",
    "    # Create time column\n",
    "    df.loc[n , 'time'] = i[2][-8:]\n",
    "    # Create upvote column\n",
    "    df.loc[n , 'upvotes'] = i[7]\n",
    "    # Create id column\n",
    "    df.loc[n , 'id'] = i[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Retrieve flair information from Reddit's API**\n",
    "\n",
    "As mentioned before, we still need to extract the flair (which indicates the topic of each submission) from each post. To do this, we will have to initiate a Reddit instance using praw (which gives access to Reddit's API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Reddit instance\n",
    "reddit = praw.Reddit(client_id='',\n",
    "                     client_secret='',\n",
    "                     user_agent='',\n",
    "                    username = '',\n",
    "                    password = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a for-loop that will check and return the appropriate flair for each submission by using its ID as verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in enumerate(df.id):\n",
    "    df.loc[a, 'topic'] = reddit.submission(id = \"{}\".format(b)).link_flair_text\n",
    "    try:\n",
    "        print(a,',', df.loc[a, 'topic'])\n",
    "    except:\n",
    "        print('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Retrieve self-text information**\n",
    "\n",
    "Most posts contain text where the user explains what their issue(s) is. Although some posts may not contain any text due to it being removed or deleted, it can still give better insight than to use only the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in enumerate(df.id):\n",
    "    df.loc[a, 'self_text'] = reddit.submission(id = \"{}\".format(b)).selftext\n",
    "    try:\n",
    "        print(a,',', df.loc[a, 'self_text'])\n",
    "    except:\n",
    "        print('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace the self-text entries where there are NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, x in enumerate(df['self_text']):\n",
    "    if pd.isnull(x):\n",
    "        df.loc[n, 'self_text'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the posts have been removed or deleted at the time that we extracted the data so they contain the strings '[removed]' and '[deleted]' inside the self-text. We will need to remove these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, x in enumerate(df['self_text']):\n",
    "    if x in ['[removed]', '[deleted]']:\n",
    "        df.loc[n, 'self_text'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concatenate the submission title and self-text**\n",
    "\n",
    "By combining the submission title and self-text for each post, we will have a better idea of what the topics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['title'] + str(' ') + df['self_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Convert Dates into datetime format**\n",
    "\n",
    "Since the data is given as a string, we will need to convert it into datatime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "for a,b in enumerate(df.date):\n",
    "     df.loc[a, 'date'] = datetime.strptime(b, '%d %b %Y ').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['date', 'time'], ascending = False).reset_index().drop('index',axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Examine basic information**\n",
    "\n",
    "Let's begin by taking a peek at the dataframe's contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe: (13001, 8)\n",
      "--------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13001 entries, 0 to 13000\n",
      "Data columns (total 8 columns):\n",
      "title        13001 non-null object\n",
      "date         13001 non-null object\n",
      "time         13001 non-null object\n",
      "upvotes      13001 non-null object\n",
      "id           13001 non-null object\n",
      "topic        12258 non-null object\n",
      "self_text    13001 non-null object\n",
      "text         13001 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 812.6+ KB\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions of the dataframe: {}'.format(df.shape))\n",
    "print(20*'-')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only column with missing data is 'topic' due to the fact that some posts have been removed and therefore their flairs no longer show up during extraction. To deal with this issue, we can simply fill in 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'].fillna('unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Debt          1563\n",
       "Other         1544\n",
       "Credit        1399\n",
       "Investing     1051\n",
       "Retirement     990\n",
       "Employment     964\n",
       "Housing        865\n",
       "Auto           743\n",
       "unknown        743\n",
       "Planning       703\n",
       "Saving         676\n",
       "Taxes          655\n",
       "Budgeting      608\n",
       "Insurance      497\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the outlier topics with the topic 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df[(df['topic'] == 'Meta' )| (df['topic'] == 'THIS IS A SPAMMER')]['topic']\n",
    "\n",
    "for id_ in outliers.index:\n",
    "    df.loc[id_,'topic'] = 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "Pre-processing is an important part of machine learning and even more significant for Natural Language Processing tasks because a simple error could result in catastrophic mishaps. \n",
    "\n",
    "There are 3 major components in data pre-processing:\n",
    "\n",
    "**1) Data-cleaning**: We will need to first clean up the text through various steps:\n",
    "\n",
    "- **Lowercase** the words so that the model will not differentiate capitalized words from other words.\n",
    "\n",
    "- **Remove numbers/digits** since the model is interpreting *text* not numbers.\n",
    "\n",
    "- **Remove punctuation** since it is not important for the context.\n",
    "\n",
    "- **Strip white space** since empty strings could be interpreted as text and we want to avoid that.\n",
    "\n",
    "- **Remove stopwords**, which are general words that are very frequent in the English dictionary (ex. because, such, so). Here is a list of some common stopwords: https://www.ranks.nl/stopwords\n",
    "\n",
    "- **Remove noise** that is not picked up through the other cleaning methods. This step can come either before or after tokenization and normalization, or both (ex. dropping words that are less than 2 characters long).\n",
    "\n",
    "**2) Tokenization**: In order to better analyze individual words, we will need to *tokenize* the documents (or in this case, the submission titles) into pieces of words. By doing so, we will be able to use the various NLP libraries to further dissect the tokens.\n",
    "\n",
    "**3) Normalization**: After tokenizing the data, we will need to normalize the text through lemmatization and stemming. Lemmatization is typically a better method since it returns the canonical forms based on a word's lemma. However, this process takes much more time compared to stemming the words, which simply removes the affixes of a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Replace the forward slash with space\n",
    "    text = text.replace('/', ' ')\n",
    "    # Remove all other punctuation without replacement\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Remove digits (excluding strings that contain both digits and letters)\n",
    "    text = ''.join([char for char in text if char not in string.digits])\n",
    "    # Strip whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stopWords])\n",
    "    # Lowercase all words\n",
    "    text = text.lower()\n",
    "    # Return only words that have more than 2 letters\n",
    "    text = ' '.join([word for word in text.split() if len(word)>2])\n",
    "    # Remove any symbols or non-alphabetical letters\n",
    "    text = re.sub('[^ a-zA-Z]', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           ways make extra side money\n",
       "1    year update legally blind going homeless one j...\n",
       "2    kicked found last night home ive staying going...\n",
       "3    online savings account hello looking recommend...\n",
       "4                      tools managing incomes expenses\n",
       "5    with resources like reddit financial consultin...\n",
       "6    credit hit late payment fee waiver score refle...\n",
       "7    need help budgetting getting debt long story s...\n",
       "8    year old male almost two year fiance recently ...\n",
       "9        debt collector gave hours pay yelled said ssn\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].apply(lambda x: preprocess(x))\n",
    "df['clean_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row number 2845 has an empty entry\n",
      "Row number 3646 has an empty entry\n",
      "Row number 4592 has an empty entry\n",
      "Row number 4705 has an empty entry\n"
     ]
    }
   ],
   "source": [
    "# Check to see all clean text are not empty\n",
    "for num, x in enumerate(df.clean_text):\n",
    "    if not x:\n",
    "        print('Row number {} has an empty entry'.format(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>????????? 1 ??? ? ?????? ??? ???????? ? 2018</td>\n",
       "      <td>2018-09-14</td>\n",
       "      <td>11:33 AM</td>\n",
       "      <td>1</td>\n",
       "      <td>9fszwz</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9597</th>\n",
       "      <td>50/20/30</td>\n",
       "      <td>2018-08-27</td>\n",
       "      <td>11:27 PM</td>\n",
       "      <td>1</td>\n",
       "      <td>9aviyl</td>\n",
       "      <td>Budgeting</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title        date      time  \\\n",
       "2167  ????????? 1 ??? ? ?????? ??? ???????? ? 2018  2018-09-14  11:33 AM   \n",
       "9597                                      50/20/30  2018-08-27  11:27 PM   \n",
       "\n",
       "      upvotes      id      topic clean_title  \n",
       "2167        1  9fszwz        NaN              \n",
       "9597        1  9aviyl  Budgeting              "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what the issue is and correct it\n",
    "df.loc[[2167, 9597]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the titles do not contain much information, let's drop them\n",
    "df.drop([2167, 9597], inplace = True)\n",
    "\n",
    "# Reset the index \n",
    "df = df.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check once more if there are any null or missing values in the 'clean_text' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2845 nan\n",
      "3646 nan\n",
      "4592 nan\n",
      "4705 nan\n"
     ]
    }
   ],
   "source": [
    "for n, x in enumerate(df['clean_text']):\n",
    "    if (pd.isnull(x)) or (not x):\n",
    "        print(n, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: 1099 or W2?, self_text: , text: 1099 or W2? , clean_text: nan\n",
      "title: 401Ks?, self_text: , text: 401Ks? , clean_text: nan\n",
      "title: 401k, self_text: , text: 401k , clean_text: nan\n",
      "title: T, self_text: , text: T , clean_text: nan\n"
     ]
    }
   ],
   "source": [
    "null_list = [2845, 3646, 4592, 4705]\n",
    "for x in null_list:\n",
    "    print('title: {}, self_text: {}, text: {}, clean_text: {}'.format(df['title'][x], df['self_text'][x], df['text'][x],\n",
    "                                                                     df['clean_text'][x]))\n",
    "df.drop(null_list, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['date', 'time'], ascending = False).reset_index().drop('index',axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Character/text Limit**\n",
    "\n",
    "Let's also limit the number of characters in the text since the longer the text, the longer the computation time. Rather than choosing an arbitrary cutoff point, we should explore various statistics regarding the text lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Text Length: 435.36116295059423\n",
      "\n",
      "Median Text length: 309.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean and median text lengths\n",
    "txt_len_mean = df['clean_text'].map(len).mean()\n",
    "txt_len_median = df['clean_text'].map(len).median()\n",
    "\n",
    "print('Mean Text Length: {}\\n\\nMedian Text length: {}'.format(txt_len_mean, txt_len_median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(858, 9254),\n",
       " (1191, 7052),\n",
       " (2435, 6278),\n",
       " (6524, 6021),\n",
       " (4495, 5515),\n",
       " (2608, 5315),\n",
       " (9846, 5172),\n",
       " (9512, 4932),\n",
       " (6921, 4926),\n",
       " (5355, 4861)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe the top 10 texts with most number of characters\n",
    "txt_len_list= {}\n",
    "for n, x in enumerate(df['clean_text']):\n",
    "    txt_len_list[n] = len(x)\n",
    "sorted(txt_len_list.items(), key = lambda x: x[1], reverse = True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35    76\n",
       "28    75\n",
       "21    74\n",
       "29    74\n",
       "30    71\n",
       "27    70\n",
       "34    68\n",
       "37    68\n",
       "23    67\n",
       "24    66\n",
       "39    65\n",
       "31    65\n",
       "26    64\n",
       "36    61\n",
       "25    60\n",
       "Name: clean_text, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 15 most frequent text length amounts\n",
    "df['clean_text'].map(len).value_counts()[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like 450 characters would be a good cut-off point. What happens when we try it out on a few samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with resources like reddit financial consulting advisement dying business seems like advantage knowing advisor breathing computer making automated decisions retirement also helps he’s professional field foreseeable future occupation',\n",
       " 'credit hit late payment fee waiver score reflect forgiveness recently forgot small charge card rarely use linked old email bills reported tanked credit score called able fees removed sure forgiveness carry credit score',\n",
       " 'need help budgetting getting debt long story short years old working full time job terrible money management issues course good amount debt feel backpedal help advice please rude comments know debt isnt lot reaching hopes reigning gets worse monthly income monthly bills rent utilities car payment car insurance phone internet gas fuel food groceries medical debt payments established payments established pay remaining this debt variety different th',\n",
       " 'year old male almost two year fiance recently dropped pursue school working full time hrs week part time hrs week fiance stepped banking job help relieve stress mom full time student honors full time job ive picking odd jobs sure make long term til school heres bills break house payment electricity water car insurance four cars planning selling one gas phone internet groceries any advice edit you guys opened mind bunch options seems like first st',\n",
       " 'debt collector gave hours pay yelled said ssn']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text[0:450] for text in df['clean_text'][5:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortuntely a few get cut-off mid-word/mid-setence so limiting by characters may not be the best option. How about if we try to limit the text length by word count instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Word Length: 64.79432275807878\n",
      "\n",
      "Median Word length: 47.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean and median word counts\n",
    "txt_wrd_mean = df['clean_text'].apply(lambda x: x.split()).map(len).mean()\n",
    "txt_wrd_median = df['clean_text'].apply(lambda x: x.split()).map(len).median()\n",
    "\n",
    "print('Mean Word Length: {}\\n\\nMedian Word length: {}'.format(txt_wrd_mean, txt_wrd_median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(858, 1338),\n",
       " (1191, 944),\n",
       " (2435, 939),\n",
       " (6524, 918),\n",
       " (2608, 819),\n",
       " (9512, 758),\n",
       " (9846, 744),\n",
       " (4495, 734),\n",
       " (6921, 732),\n",
       " (9090, 688)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe the top 10 texts with most number of word\n",
    "txt_wrd_list= {}\n",
    "for n, x in enumerate(df['clean_text']):\n",
    "    txt_wrd_list[n] = len(x.split())\n",
    "sorted(txt_wrd_list.items(), key = lambda x: x[1], reverse = True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of texts with word count less than 150: 90.11%\n"
     ]
    }
   ],
   "source": [
    "# Count the number of texts with fewer than 150 words\n",
    "word_count_ = len(df[df['clean_text'].apply(lambda x: x.split()).map(len)<150])\n",
    "total_len = len(df)\n",
    "\n",
    "print('Percentage of texts with word count less than 150: {0:.2f}%'.format((word_count_/total_len)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ways make extra side money\n",
      "\n",
      "year update legally blind going homeless one job making month please help\n",
      "\n",
      "kicked found last night home ive staying going sale days currently working due work related injury returning make week mostly appartments around high mid range currently working car sure subreddit tips looking places live bugdet money properly anything else thanks edit grammer errors\n",
      "\n",
      "online savings account hello looking recommendations online savings account basically something easily transfer money limits taking money working changing spending habits would like commit certain percentage income every pay period still working getting debts paid first total thankfully adjusting better habits like bringing lunch instead ordering thank\n",
      "\n",
      "tools managing incomes expenses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in df['clean_text'][0:5]:\n",
    "    print(' '.join(text.split()[0:150]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = [' '.join(text.split()[0:150]) for text in df['clean_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the data as a .csv file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .csv file\n",
    "df.to_csv(r'C:\\Users\\joshua\\Downloads\\Data\\reddit\\reddit_pf3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\joshua\\Downloads\\Data\\reddit\\reddit_pf3.csv', engine='python', index_col=[0], parse_dates = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tokenization**\n",
    "\n",
    "Tokenization is essentially the process of segmenting a text into pieces, such as words, phrases, symbols, etc. \n",
    "\n",
    "Let's create a list of the tokens for each submission title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>self_text</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>9h6whn</td>\n",
       "      <td></td>\n",
       "      <td>Ways to make extra side money?</td>\n",
       "      <td>12:57 PM</td>\n",
       "      <td>Ways to make extra side money?</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>ways make extra side money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>9h29g7</td>\n",
       "      <td></td>\n",
       "      <td>(Year UPDATE) Legally blind, going homeless, h...</td>\n",
       "      <td>12:56 AM</td>\n",
       "      <td>(Year UPDATE) Legally blind, going homeless, h...</td>\n",
       "      <td>Other</td>\n",
       "      <td>16</td>\n",
       "      <td>year update legally blind going homeless one j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>9h6vyv</td>\n",
       "      <td>So i just found out last night the home ive be...</td>\n",
       "      <td>19, being kicked out So i just found out last ...</td>\n",
       "      <td>12:55 PM</td>\n",
       "      <td>19, being kicked out</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "      <td>kicked found last night home ive staying going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>9h6vs4</td>\n",
       "      <td>Hello! Looking for recommendations for an onli...</td>\n",
       "      <td>Online Savings Account? Hello! Looking for rec...</td>\n",
       "      <td>12:54 PM</td>\n",
       "      <td>Online Savings Account?</td>\n",
       "      <td>Saving</td>\n",
       "      <td>1</td>\n",
       "      <td>online savings account hello looking recommend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>9h6v48</td>\n",
       "      <td></td>\n",
       "      <td>Tools for Managing Incomes and Expenses</td>\n",
       "      <td>12:52 PM</td>\n",
       "      <td>Tools for Managing Incomes and Expenses</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>tools managing incomes expenses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      id                                          self_text  \\\n",
       "0  2018-09-19  9h6whn                                                      \n",
       "1  2018-09-19  9h29g7                                                      \n",
       "2  2018-09-19  9h6vyv  So i just found out last night the home ive be...   \n",
       "3  2018-09-19  9h6vs4  Hello! Looking for recommendations for an onli...   \n",
       "4  2018-09-19  9h6v48                                                      \n",
       "\n",
       "                                                text      time  \\\n",
       "0                    Ways to make extra side money?   12:57 PM   \n",
       "1  (Year UPDATE) Legally blind, going homeless, h...  12:56 AM   \n",
       "2  19, being kicked out So i just found out last ...  12:55 PM   \n",
       "3  Online Savings Account? Hello! Looking for rec...  12:54 PM   \n",
       "4           Tools for Managing Incomes and Expenses   12:52 PM   \n",
       "\n",
       "                                               title    topic  upvotes  \\\n",
       "0                     Ways to make extra side money?  unknown        1   \n",
       "1  (Year UPDATE) Legally blind, going homeless, h...    Other       16   \n",
       "2                               19, being kicked out    Other        2   \n",
       "3                            Online Savings Account?   Saving        1   \n",
       "4            Tools for Managing Incomes and Expenses    Other        0   \n",
       "\n",
       "                                          clean_text  \n",
       "0                         ways make extra side money  \n",
       "1  year update legally blind going homeless one j...  \n",
       "2  kicked found last night home ive staying going...  \n",
       "3  online savings account hello looking recommend...  \n",
       "4                    tools managing incomes expenses  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def create_tokens(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token for token in doc]\n",
    "    return tokens\n",
    "\n",
    "df['tokenized_text'] = df['clean_text'].apply(lambda x: create_tokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lemmatization**\n",
    "\n",
    "Lemmas are the \"base form\" of a word. \n",
    "\n",
    "Ex. walk, walked, walking, walks would all be derived from the base form 'walk'. \n",
    "\n",
    "Using the tokens that we generated in the column 'tokenized_title', let's next lemmatize the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    # Make sure to remove pronouns (ex. he, she) before returning the lemmas\n",
    "    lemmas = [token.lemma_ for token in text if token.lemma_ not in '-PRON-']\n",
    "    return lemmas\n",
    "\n",
    "df['lemmatized_text'] = df['tokenized_text'].apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Named Entity Recognition**\n",
    "\n",
    "Named entities are real-world objects that have a name, such as a person, country, or company. spaCy is able to recognize different types of named entities in a document and can return features such as the label (ex. ORG - organization, GPE - geopolitical entity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NER(text, label = False):\n",
    "    doc = nlp(text)\n",
    "    if label is False:\n",
    "        NER_list = [(ent.text) for ent in doc.ents]\n",
    "    else:\n",
    "        NER_list = [(ent.label_) for ent in doc.ents]\n",
    "    return NER_list    \n",
    "\n",
    "df['named_entities'] = df['clean_text'].apply(lambda x: create_NER(x))\n",
    "df['entity_labels'] = df['clean_text'].apply(lambda x: create_NER(x, label = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop('tokenized_text', axis=1).to_pickle(r'C:\\Users\\Joshua\\Pickle_files\\df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
