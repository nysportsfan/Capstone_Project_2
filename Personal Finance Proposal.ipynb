{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 2 Proposal\n",
    "\n",
    "Joshua Kim\n",
    "\n",
    "Springboard Data Science Career Track\n",
    "\n",
    "### 1) Project Name:\n",
    "\n",
    "Personal Finance Subreddit Analysis\n",
    "\n",
    "### 2) Problem Statement:\n",
    "\n",
    "One of the common problems that the everyday American faces is their financial situation. There are many different reasons for this, such as debt, student loans, the passing of a parent. According to a study of 1000 Americans over the age of 30 (conducted by GuideVine), less than half of Americans understood common financial concepts such as interest, bankruptcy, and inflation. More importantly, more than half felt \"lost\" when it came to having a secure long-term plan for their financial future. This highlights a re-occuring theme in our society where many people have trouble getting by with their finances and naturally some people seek help online. One of these resources is a subreddit called 'Personal Finance', which is hosted on the popular discussion-board website 'Reddit'. \n",
    "\n",
    "/r/personalfinance has become a place where many people are able to ask for help regarding their financial situations. A few common topics are savings, budgeting, taxes and debt. There are hundreds of different posts everyday from people of different ages, genders, locations and occupations. Frequent users of the subreddit often provide advice through comments. Using the information provided by the posts, can we discover trends and common themes? \n",
    "\n",
    "### 3) Clients:\n",
    "\n",
    "Consumer-based companies in the financial sector: There are many different kinds of problems that occur in this subreddit and many of these companies would be interested in understanding key trends about their potential clients. For example, a credit card company may want to learn about how often a millenial has a dilemna with paying back their credit card bills. \n",
    "\n",
    "### 4) Data Sets:\n",
    "\n",
    "For this report, I will be creating my own dataset through 2 steps:\n",
    "\n",
    "- First, I will extract the submission information using the pushshift.io Reddit API (https://github.com/pushshift/api). It will give us the ability to search through Reddit data and create data aggregations. Using sqlite will be necessary as there will be many different rows of data.\n",
    "\n",
    "\n",
    "- Second, I will use the official Reddit API to extract information that is not provided through the pushshift.io Reddit API. In this case, we will be extracting the flair information (which will act as the \"topics\" of each submission). The reason we cannot simply use the official Reddit API to extract all the submission data is because they limit the number of submissions (1000) you are allowed to extract as well as the date range. \n",
    "\n",
    "    - For example, if we extract the 1000 most recent posts on a subreddit, we cannot extract the next 1000 most recent posts since we cannot specify the date range. \n",
    "\n",
    "Due to the extremely taxing and computationally expensive length of this procedure, we will have a dataset that contains approximately 100,000 rows of data. Each row represents a submission in the Personal Finance subreddit. The features will include:\n",
    "\n",
    "- title\n",
    "- date\n",
    "- time\n",
    "- number of upvotes\n",
    "- id\n",
    "- topic (flair)\n",
    "\n",
    "### 5) Methodology:\n",
    "\n",
    "To begin, I will first clean the data through data wrangling. I will then perform feature engineering by using various Natural Language Processing tools (nltk, gensim, spaCy). This will include tokenization, removing stopwords, part-of-speech (POS) tagging, etc. We will then use data exploration to create visualizations and observe various trends in the dataframes (Note: NLP projects typically do not lend themselves to much EDA due to the lack of variety in the features). The next step will be to perform inferential statistics to select the features for implementation in the models. Finally, we will be creating 2 main types of Machine learning models:\n",
    "\n",
    "1) Supervised Learning: We will use the features that we have created via tokenization and vectorization in order to predict the topic of a given submission. The different models we might use include Multinomial Logistic Regression, Random Forest Classifier, and Gradient Boosting.\n",
    "\n",
    "2) Unsupervised Learning: We will also be creating a Latent Dirichlet allocation model, which generative statistical model that's often used for topic modeling. In this case, we will be creating topics based on the given features rather than predicting the topics like in the supervised learning model.\n",
    "\n",
    "### 6) Deliverables:\n",
    "\n",
    "Deliverables will include the code (data wrangling, EDA, machine learning) and the final report (abstract report and powerpoint presentation) on GitHub. Another potential deliverable could be a blog post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
