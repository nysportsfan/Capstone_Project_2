{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Finance Subreddit Capstone Project\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "The goal of this notebook is to extract data containing information about the personal finance subreddit from Reddit's API, select interesting features that can be relevant to the project and clean the dataset so that it is ready for data exploration.\n",
    "\n",
    "- **Import the necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import string\n",
    "import time\n",
    "import datetime\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Extract the data from reddit using pushshift**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "sub    = 'personalfinance'     # name of the subreddit you would like to scrape\n",
    "after  = '2018-08-01'    # earliest date that will be scraped\n",
    "before = '2018-09-19'    # latest date that will be scraped\n",
    "fast   = True           # True will be faster, won't pull upvote ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate sqlite\n",
    "sql = sqlite3.connect('personalfinance_.db')\n",
    "cur = sql.cursor()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS posts (name TEXT, title TEXT, readable_utc TEXT, permalink TEXT, domain TEXT, url TEXT, author TEXT, score TEXT, upvote_ratio TEXT, num_comments TEXT)')\n",
    "sql.commit()\n",
    "print('Loaded SQL Database and Tables')\n",
    "\n",
    "# Convert the specified dates to strptime\n",
    "after = time.mktime(datetime.datetime.strptime(after, '%Y-%m-%d').timetuple())\n",
    "after = int(after)\n",
    "readable_after = time.strftime('%d %b %Y %I:%M %p', time.localtime(after))\n",
    "before = time.mktime(datetime.datetime.strptime(before, '%Y-%m-%d').timetuple())\n",
    "before = int(before) + 86399\n",
    "readable_before = time.strftime('%d %b %Y %I:%M %p', time.localtime(before))\n",
    "print('Searching for posts between ' + readable_after + ' and ' + readable_before + '.')\n",
    "currentDate = before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pushshift will allow us to retrieve valuable information from reddit submissions including:\n",
    "- Submission ID\n",
    "- Submission title\n",
    "- Submission date\n",
    "- Submission permalink\n",
    "- Submission domain\n",
    "- Submission url\n",
    "- Submission author\n",
    "- Submission score (upvotes)\n",
    "- Submission upvote_ratio (ratio of upvotes to downvotes)\n",
    "- Submission number of comments\n",
    "\n",
    "However, it does not provide us with the flair information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a new full pull from Pushshift\n",
    "def newpull(thisBefore):\n",
    "    global currentDate\n",
    "    readable_thisBefore = time.strftime('%d %b %Y %I:%M %p', time.localtime(thisBefore))\n",
    "    print('Searching posts before ' + str(readable_thisBefore))\n",
    "    url = 'http://api.pushshift.io/reddit/search/submission/?subreddit=' + sub + '&size=500&before=' + str(thisBefore)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print('    Discussion: HTML Error - ', response.status_code)\n",
    "        time.sleep(60)\n",
    "        return\n",
    "    curJSON = response.json()\n",
    "\n",
    "    # Update each Pushshift result with Reddit data\n",
    "    for child in curJSON['data']:\n",
    "\n",
    "        # Check to see if already added\n",
    "        name = str(child['id'])\n",
    "        cur.execute('SELECT * FROM posts WHERE name == ?', [name])\n",
    "        if cur.fetchone():\n",
    "            print(str(child['id']) + ' skipped (already in database)')\n",
    "            continue\n",
    "\n",
    "        # If not, get more data\n",
    "        if fast is True:\n",
    "            searchURL = 'http://reddit.com/by_id/t3_'\n",
    "        else:\n",
    "            searchURL = 'http://reddit.com/'\n",
    "        url = searchURL + str(name) + '.json'\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print('    Discussion: HTML Error - ', response.status_code)\n",
    "            time.sleep(60)\n",
    "            break\n",
    "        postJSON = response.json()\n",
    "        if fast is True:\n",
    "            jsonStart = postJSON\n",
    "        else:\n",
    "            jsonStart = postJSON[0]\n",
    "\n",
    "        # Check to see if Date has passed\n",
    "        global currentDate\n",
    "        created_utc = jsonStart['data']['children'][0]['data']['created_utc']\n",
    "        currentDate = int(created_utc)\n",
    "        if currentDate <= after:\n",
    "            break\n",
    "\n",
    "        # If not, process remaining data\n",
    "        try:\n",
    "            title = str(jsonStart['data']['children'][0]['data']['title'])  # Checks for emojis and other non-printable characters\n",
    "        except UnicodeEncodeError:\n",
    "            title = ''.join(c for c in str(jsonStart['data']['children'][0]['data']['title']) if c in string.printable)\n",
    "        readable_utc = time.strftime('%d %b %Y %I:%M %p', time.localtime(created_utc))\n",
    "        permalink    = (str(jsonStart['data']['children'][0]['data']['permalink']))\n",
    "        domain       = (str(jsonStart['data']['children'][0]['data']['domain']))\n",
    "        url          = (str(jsonStart['data']['children'][0]['data']['url']))\n",
    "        author       = (str(jsonStart['data']['children'][0]['data']['author']))\n",
    "        score        = (str(jsonStart['data']['children'][0]['data']['score']))\n",
    "        num_comments = (str(jsonStart['data']['children'][0]['data']['num_comments']))\n",
    "        if fast is True:\n",
    "            upvote_ratio = 0\n",
    "        else:\n",
    "            upvote_ratio = (str(jsonStart['data']['children'][0]['data']['upvote_ratio']))\n",
    "\n",
    "        # Write it to SQL Database\n",
    "        cur.execute('INSERT INTO posts VALUES(?,?,?,?,?,?,?,?,?,?)', [name, title, readable_utc, permalink, domain, url, author, score, upvote_ratio, num_comments])\n",
    "        sql.commit()\n",
    "\n",
    "# Run the newpull\n",
    "while currentDate >= after:\n",
    "    newpull(currentDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have pulled the data from pushshift, we will need to create a dataframe which will store the relevant information (title, date, time, upvotes, id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconnect to sqlite\n",
    "connection = sqlite3.connect(\"personalfinance_.db\") \n",
    "  \n",
    "# Cursor object \n",
    "crsr = connection.cursor() \n",
    "  \n",
    "# Execute the command to fetch all the data from the table posts \n",
    "crsr.execute(\"SELECT * FROM posts\")  \n",
    "  \n",
    "# Store all the fetched data in the ans variable \n",
    "ans= crsr.fetchall()  \n",
    "\n",
    "# Create empty dataframe\n",
    "columns = ['title']\n",
    "index = range(0,2)\n",
    "df = pd.DataFrame(index = index, columns = columns)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Create new columns and extract the relevant data \n",
    "for n, i in enumerate(ans):\n",
    "    # Create title column\n",
    "    df.loc[n , 'title'] = i[1]\n",
    "    # Create date column\n",
    "    df.loc[n , 'date'] = i[2][:-8]\n",
    "    # Create time column\n",
    "    df.loc[n , 'time'] = i[2][-8:]\n",
    "    # Create upvote column\n",
    "    df.loc[n , 'upvotes'] = i[7]\n",
    "    # Create id column\n",
    "    df.loc[n , 'id'] = i[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Retrieve flair information from Reddit's API**\n",
    "\n",
    "As mentioned before, we still need to extract the flair (which indicates the topic of each submission) from each post. To do this, we will have to initiate a Reddit instance using praw (which gives access to Reddit's API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Reddit instance\n",
    "reddit = praw.Reddit(client_id='',\n",
    "                     client_secret='',\n",
    "                     user_agent='',\n",
    "                    username = '',\n",
    "                    password = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a for-loop that will check and return the appropriate flair for each submission by using its ID as verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in enumerate(df.id):\n",
    "    df.loc[a, 'topic'] = reddit.submission(id = \"{}\".format(b)).link_flair_text\n",
    "    try:\n",
    "        print(a,',', df.loc[a, 'topic'])\n",
    "    except:\n",
    "        print('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Convert Dates into datetime format**\n",
    "\n",
    "Since the data is given as a string, we will need to convert it into datatime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "for a,b in enumerate(df.date):\n",
    "     df.loc[a, 'date'] = datetime.strptime(b, '%d %b %Y ').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['date', 'time'], ascending = False).reset_index().drop('index',axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Examine basic information**\n",
    "\n",
    "Let's begin by taking a peek at the dataframe's contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe: (10183, 6)\n",
      "--------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10183 entries, 0 to 10182\n",
      "Data columns (total 6 columns):\n",
      "title      10183 non-null object\n",
      "date       10183 non-null object\n",
      "time       10183 non-null object\n",
      "upvotes    10183 non-null int64\n",
      "id         10183 non-null object\n",
      "topic      9532 non-null object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 556.9+ KB\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions of the dataframe: {}'.format(df.shape))\n",
    "print(20*'-')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only column with missing data is 'topic' due to the fact that some posts have been removed and therefore their flairs no longer show up during extraction. To deal with this issue, we can simply fill in 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'].fillna('unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the data as a .csv file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .csv file\n",
    "df.to_csv(r'C:\\Users\\joshu\\Downloads\\Data\\reddit\\reddit_pf1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
